{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a267e0f9-b4b8-45ab-9d70-d7045571d9e6",
   "metadata": {},
   "source": [
    "# Merging processed data\n",
    "This notebook relies on the data from the previous notebook (but there is no need to run the previous notebook for this one to work however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fedbcaf-dde3-436d-bf2d-5cc0b24129a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Basemap package cannot be imported | No module named 'mpl_toolkits.basemap'  error detected - Groundtrack plot disabled!\n"
     ]
    }
   ],
   "source": [
    "import gnssvod as gv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da3daa-f534-4957-8d4c-09d7ec50d67b",
   "metadata": {},
   "source": [
    "## Merge\n",
    "In the previous notebook, we processed raw RINEX observation files individually for each receiver and saved the results in corresponding NetCDF files.\n",
    "\n",
    "In the case of a GNSS-VOD set up, receivers are analysed as pairs. One receiver lies above the forest canopy and provides a clear-sky reference, and the other one lies below the canopy and measures the forest attenuation.\n",
    "\n",
    "Here we merge the data from these two receivers before making any plots. We also save the merged data in chunks that are always the same (for example we save them in daily chunks). This makes it easier to manipulate data and avoids relying on the temporal chunks with which data was initially logged (here data was logged in hourly log files that span from xx:07 too xx+1:06).\n",
    "\n",
    "### gv.gather_stations()\n",
    "This function will do several things\n",
    "- It will read processed observation files that were saved in NetCDF format (output of \"preprocess\").\n",
    "- It will combine data from the various receivers/stations according to user-specified pairing rules.\n",
    "- It will only process data belonging to the requested time interval.\n",
    "- It will return and/or save paired data in temporal chunks specified by the time interval.\n",
    "\n",
    "#### Specifying input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2e95bb-d5a9-4887-9206-aec3223f8fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's indicate where to find the data for each receiver\n",
    "pattern={'MACROCOSM-5':'data_pr/nc/MACROCOSM-5*.nc',\n",
    "        'MACROCOSM-2': 'data_pr/nc/MACROCOSM-2*.nc'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dc425-b818-4ec8-b893-e5387ea2c0fa",
   "metadata": {},
   "source": [
    "#### Specifying time interval\n",
    "Then we need to define the temporal interval and the temporal chunks we will want for the output data\n",
    "                                                                             \n",
    "Here we decide to process all data from '28-04-2021' to '29-04-2021', meaning 2 days, starting at '28-04-2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7bcab7-266b-4e47-b016-bb66f5b6a87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalIndex([[2024-01-10 00:00:00, 2024-01-11 00:00:00), [2024-01-11 00:00:00, 2024-01-12 00:00:00)], dtype='interval[datetime64[ns], left]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startday = start=pd.to_datetime('10-01-2024',format='%d-%m-%Y')\n",
    "timeintervals=pd.interval_range(start=startday, periods=2, freq='D', closed='left')\n",
    "timeintervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716a18c-aec0-4f30-b143-3ccd6cf73218",
   "metadata": {},
   "source": [
    "Using the timeintervals above will save/return the results in chunks of 1 day. If we wanted the results in hourly chunks, we could have written instead:\n",
    "\n",
    "`timeintervals=pd.interval_range(start=startday, periods=48, freq='H', closed='left')`\n",
    "\n",
    "Now the only thing left is to define how to combine the stations, using the same dictionary keys as in 'pattern'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56232ec2-f8e8-4421-ad97-6400919e88a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MACROCOSM\n",
      "Listing the files matching with the interval\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unrecognized chunk manager dask - must be one of: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pairings\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACROCOSM\u001b[39m\u001b[38;5;124m'\u001b[39m:(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACROCOSM-5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACROCOSM-2\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# run function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mgv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_stations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpairings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtimeintervals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gnssvod/io/preprocess.py:314\u001b[0m, in \u001b[0;36mgather_stations\u001b[0;34m(filepattern, pairings, timeintervals, keepvars, outputdir, compress)\u001b[0m\n\u001b[1;32m    311\u001b[0m iout \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m station_name \u001b[38;5;129;01min\u001b[39;00m station_names:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# get Epochs from all files\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m [\u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mEpoch \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m filenames[station_name]]\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# check which files have data that overlaps with the desired time intervals\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     isin \u001b[38;5;241m=\u001b[39m [overall_interval\u001b[38;5;241m.\u001b[39moverlaps(pd\u001b[38;5;241m.\u001b[39mInterval(left\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mTimestamp(x\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mmin()),\n\u001b[1;32m    317\u001b[0m                                                   right\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mTimestamp(x\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mmax()))) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m epochs]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xarray/backends/api.py:1077\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m   1075\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m-> 1077\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m   1078\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xarray/backends/api.py:594\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    588\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    589\u001b[0m     filename_or_obj,\n\u001b[1;32m    590\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    593\u001b[0m )\n\u001b[0;32m--> 594\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43m_dataset_from_backend_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xarray/backends/api.py:370\u001b[0m, in \u001b[0;36m_dataset_from_backend_dataset\u001b[0;34m(backend_ds, filename_or_obj, engine, chunks, cache, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, **extra_tokens)\u001b[0m\n\u001b[1;32m    368\u001b[0m     ds \u001b[38;5;241m=\u001b[39m backend_ds\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43m_chunk_ds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite_encoded_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m ds\u001b[38;5;241m.\u001b[39mset_close(backend_ds\u001b[38;5;241m.\u001b[39m_close)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Ensure source filename always stored in dataset object\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xarray/backends/api.py:318\u001b[0m, in \u001b[0;36m_chunk_ds\u001b[0;34m(backend_ds, filename_or_obj, engine, chunks, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, **extra_tokens)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chunk_ds\u001b[39m(\n\u001b[1;32m    308\u001b[0m     backend_ds,\n\u001b[1;32m    309\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_tokens,\n\u001b[1;32m    317\u001b[0m ):\n\u001b[0;32m--> 318\u001b[0m     chunkmanager \u001b[38;5;241m=\u001b[39m \u001b[43mguess_chunkmanager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# TODO refactor to move this dask-specific logic inside the DaskManager class\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunkmanager, DaskManager):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xarray/namedarray/parallelcompat.py:117\u001b[0m, in \u001b[0;36mguess_chunkmanager\u001b[0;34m(manager)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(manager, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m chunkmanagers:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munrecognized chunk manager \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - must be one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(chunkmanagers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunkmanagers[manager]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(manager, ChunkManagerEntrypoint):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# already a valid ChunkManager so just pass through\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: unrecognized chunk manager dask - must be one of: []"
     ]
    }
   ],
   "source": [
    "# define how to make pairs, always give reference station first, matching the dictionary keys of 'pattern'\n",
    "pairings={'MACROCOSM':('MACROCOSM-5','MACROCOSM-2')}\n",
    "\n",
    "# run function\n",
    "out = gv.gather_stations(pattern,pairings,timeintervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc79c1-19e9-44d3-9960-d4b71b8bf251",
   "metadata": {},
   "source": [
    "The result is of the form\n",
    "\n",
    "out = dict(key:list(\n",
    "<br>&emsp;&emsp;tuple(pd.Interval,pd.DataFrame)),\n",
    "<br>&emsp;&emsp;tuple(pd.Interval,pd.DataFrame)),\n",
    "<br>&emsp;&emsp;tuple(pd.Interval,pd.DataFrame))\n",
    "<br>)\n",
    "\n",
    "In our case, something like:\n",
    "\n",
    "out = dict('Dav': \\[\n",
    "<br>&emsp;&emsp;(Interval('2021-04-28', '2021-04-29', closed='left'), dataframe),\n",
    "<br>&emsp;&emsp;(Interval('2021-04-29', '2021-04-30', closed='left'), dataframe)\n",
    "<br>\\])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fad3763-17c3-402d-a1a0-505388754fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S7</th>\n",
       "      <th>Azimuth</th>\n",
       "      <th>Elevation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Station</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>SV</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Dav2_Twr</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2021-04-28 21:07:00</th>\n",
       "      <th>C06</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>36.600002</td>\n",
       "      <td>10.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C09</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>32.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C11</th>\n",
       "      <td>43.400002</td>\n",
       "      <td>43.400002</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>177.199997</td>\n",
       "      <td>35.100002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C14</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>42.299999</td>\n",
       "      <td>-96.400002</td>\n",
       "      <td>76.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C16</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>38.299999</td>\n",
       "      <td>15.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Dav1_Grnd</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2021-04-28 23:59:45</th>\n",
       "      <th>R20</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>35.299999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-81.099998</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R21</th>\n",
       "      <td>33.600002</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21.200001</td>\n",
       "      <td>14.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S23</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S27</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S36</th>\n",
       "      <td>32.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43172 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          S1         S2         S7  \\\n",
       "Station   Epoch               SV                                     \n",
       "Dav2_Twr  2021-04-28 21:07:00 C06  38.000000  38.000000  31.000000   \n",
       "                              C09  41.000000  41.000000  36.000000   \n",
       "                              C11  43.400002  43.400002  41.000000   \n",
       "                              C14  45.000000  45.000000  42.299999   \n",
       "                              C16  38.000000  38.000000  33.000000   \n",
       "...                                      ...        ...        ...   \n",
       "Dav1_Grnd 2021-04-28 23:59:45 R20  39.000000  35.299999        NaN   \n",
       "                              R21  33.600002  34.000000        NaN   \n",
       "                              S23  35.000000        NaN        NaN   \n",
       "                              S27  31.000000        NaN        NaN   \n",
       "                              S36  32.500000        NaN        NaN   \n",
       "\n",
       "                                      Azimuth  Elevation  \n",
       "Station   Epoch               SV                          \n",
       "Dav2_Twr  2021-04-28 21:07:00 C06   36.600002  10.100000  \n",
       "                              C09   49.000000  32.700001  \n",
       "                              C11  177.199997  35.100002  \n",
       "                              C14  -96.400002  76.800003  \n",
       "                              C16   38.299999  15.200000  \n",
       "...                                       ...        ...  \n",
       "Dav1_Grnd 2021-04-28 23:59:45 R20  -81.099998  43.000000  \n",
       "                              R21  -21.200001  14.700000  \n",
       "                              S23         NaN        NaN  \n",
       "                              S27         NaN        NaN  \n",
       "                              S36         NaN        NaN  \n",
       "\n",
       "[43172 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['MACROCOSM'][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f5f97-80f1-4f32-9358-2142a36fb755",
   "metadata": {},
   "source": [
    "#### Specifying output destination\n",
    "Instead of just returning the result as an output of the function, we can specify where to save it instead. Again it may also be useful to get rid of some variables that are not useful to reduce file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351245e9-b575-4433-a78f-fc3e08130672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dav\n",
      "Listing the files matching with the interval\n",
      "Found 6 files for Dav2_Twr\n",
      "Reading\n",
      "Found 6 files for Dav1_Grnd\n",
      "Reading\n",
      "Concatenating\n",
      "Saving files for Dav in data_RINEX2.11/Dav_paired/\n",
      "Saved 43172 obs in Dav_20210428000000_20210429000000.nc\n",
      "Saved 46164 obs in Dav_20210429000000_20210430000000.nc\n"
     ]
    }
   ],
   "source": [
    "# define where to save output data, matching the dictionary keys in 'pairings'\n",
    "outputdir = {'Dav':'data_RINEX2.11/Dav_paired/'}\n",
    "# define which variables to keep\n",
    "keepvars = ['S1','S2','Azimuth','Elevation']\n",
    "\n",
    "# run function\n",
    "out = gv.gather_stations(pattern,pairings,timeintervals,keepvars=keepvars,outputdir=outputdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14ccda-7cde-4ece-a465-be4114a08d6a",
   "metadata": {},
   "source": [
    "As we asked, the results have been saved as daily files (even though the input files are hourly files). The file names are generated based on the key of the 'pairing' argument (here 'Dav') and the specified time intervals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
